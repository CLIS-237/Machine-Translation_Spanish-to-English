{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq_seq_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bJC27ibhXVGWPSvgXOfdrdIvfyjx6W_8",
      "authorship_tag": "ABX9TyM/BrgNiDsQqMq+Nr/YEcaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunshyPiKaChew/seq2seq_attention/blob/master/seq_seq_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnWPdM__tQJc",
        "outputId": "6b5f1584-c834-42f5-8855-154670d54797"
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/43/dcf19f30fb1da9e971e591d95e56c7b9d4e417f4a203ad272eb746742f42/tensorflow-2.0.0b1-cp37-cp37m-manylinux1_x86_64.whl (88.7MB)\n",
            "\u001b[K     |████████████████████████████████| 88.7MB 65kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.19.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.32.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (3.12.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.3.3)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.36.2)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta1) (56.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.10.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.4.1)\n",
            "Installing collected packages: keras-applications, tf-estimator-nightly, tb-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMzpjEWgtQXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d250849c-eff4-4fe6-f1d8-79315aa1de57"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLixUxKutcl2",
        "outputId": "cc6282fb-db1d-4468-9aff-43de73f63a5c"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "  print(module.__name__, module.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-beta1\n",
            "sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.2\n",
            "numpy 1.19.5\n",
            "pandas 1.1.5\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.0.0-beta1\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVyjESk2tzDv"
      },
      "source": [
        "# 1. preprocessing data\n",
        "# 2. build model\n",
        "# 2.1 encoder\n",
        "# 2.2 attetion\n",
        "# 2.3 decoder\n",
        "# 3. evaluation\n",
        "# 3.1 given sentence. return translate results\n",
        "# 3.2 visualize results (attention)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6hEAeCUvzlo",
        "outputId": "705d644d-777d-4ec8-ef6c-d75fff72da1b"
      },
      "source": [
        "# unicode2ascii 去掉西班牙语的重音\n",
        "import unicodedata\n",
        "def unicode_to_ascii(s):\n",
        "  # normalize 的 NFD 方法，如果一个unicode值包含多个字符，那么把他拆开，例如e和重音分开\n",
        "  # 'Mn' 重音的分类标志\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "en_sentence = 'Then what?'\n",
        "sp_sentence = '¿Entonces qué?'\n",
        "\n",
        "print(unicode_to_ascii(en_sentence))\n",
        "print(unicode_to_ascii(sp_sentence))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Then what?\n",
            "¿Entonces que?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSq4oDIhypbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27e9d2f3-a0a9-4520-fb4b-8234bde61cfc"
      },
      "source": [
        "# 字符串预处理\n",
        "import re\n",
        "def preprocess_sentence(s):\n",
        "  s = unicode_to_ascii(s.lower().strip())\n",
        "  # [] 匹配操作 () 替换操作 前后加空格\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  # 将一个或多个空格替换为一个空格\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  # 除了字母和标点符号都替换为空格\n",
        "  s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
        "  # 去掉前后的空格\n",
        "  s = s.rstrip().strip()\n",
        "  # 添加前后特殊字符\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n",
        "\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> then what ? <end>\n",
            "<start> ¿ entonces que ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1orHnX7vNGD",
        "outputId": "ebcc5560-aee9-4133-9f52-7d4da3d3655e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "en_spa_file_path = '/content/drive/MyDrive/Colab/chapter_10/data_spa_en/spa.txt'"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q1aGPzXjdGQ",
        "outputId": "b77f5fa9-c158-4be2-9b3c-5c3d0ff0fbe1"
      },
      "source": [
        "def parse_data(filename):\n",
        "  # 根据回车分割数据中的每一行\n",
        "  lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
        "  # 根据制表符分割西班牙文和英文\n",
        "  sentence_pairs = [line.split('\\t') for line in lines]\n",
        "  preprocessed_sentence_pairs = [\n",
        "    (preprocess_sentence(en), preprocess_sentence(sp)) for en,sp in sentence_pairs]\n",
        "  return zip(*preprocessed_sentence_pairs)\n",
        "\n",
        "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
        "print(en_dataset[-1])\n",
        "print(sp_dataset[-1])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF7iAng9yziM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72669f1b-a4a2-4d2b-af73-1f433a71ccc6"
      },
      "source": [
        "# 文本式数据转化为ID式数据\n",
        "def tokenizer(lang):\n",
        "  lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words = None, filters='', split=' ')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  # Padding\n",
        "  tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
        "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
        "\n",
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "max_length_input = max_length(input_tensor)\n",
        "max_length_output = max_length(output_tensor)\n",
        "print(max_length_input, max_length_output)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3at6ZzSc_qZs",
        "outputId": "36aa5213-a6b5-4cb6-8e0f-e3a3e2d5bbf5"
      },
      "source": [
        "# 调用sklearn函数分割数据集\n",
        "from sklearn.model_selection import train_test_split\n",
        "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size = 0.2)\n",
        "len(input_train),len(input_eval), len(output_train), len(output_eval)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 6000, 24000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA5l5F6iApKQ",
        "outputId": "4dc62e47-5e5f-4e4b-992d-4fb74592d73b"
      },
      "source": [
        "def convert(example, tokenizer):\n",
        "  for t in example:\n",
        "    if t != 0:\n",
        "      print('%d --> %s' % (t,tokenizer.index_word[t]))\n",
        "\n",
        "convert(input_train[0],input_tokenizer)\n",
        "print()\n",
        "convert(output_train[0],output_tokenizer)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 --> <start>\n",
            "12 --> me\n",
            "380 --> quieren\n",
            "10 --> a\n",
            "19 --> mi\n",
            "3 --> .\n",
            "2 --> <end>\n",
            "\n",
            "1 --> <start>\n",
            "28 --> they\n",
            "47 --> want\n",
            "17 --> me\n",
            "3 --> .\n",
            "2 --> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J32VumKI-L7N",
        "outputId": "8f7b9e10-05c2-45e2-f1f5-585ff1b24cb6"
      },
      "source": [
        "# 比如训练集有50000个样本，而我设定的batch_size是50，也就是说每50个样本才更新一次参数，那么也就意味着一个epoch里会提取1000次bach，\n",
        "# 这样才会把每个样本都提取了一遍，更新了1000次参数。\n",
        "\n",
        "# 这是一个epoch里做的，依次类推，我要设定2000个epoch意味着把这个过程重复2000次。也就是训练集里的每个样本都被提取了2000次。\n",
        "\n",
        "# 生成DataSet\n",
        "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(30000)\n",
        "  dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
        "  return dataset\n",
        "\n",
        "batch_size = 64;\n",
        "epochs = 20\n",
        "\n",
        "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
        "eval_dataset = make_dataset(input_eval, output_eval, batch_size, epochs, False)\n",
        "\n",
        "# 64 是一个bacth的大小 16 11 分别为输入输出padding之后的大小\n",
        "for x,y in train_dataset.take(1):\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  print(x)\n",
        "  print(y)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 16)\n",
            "(64, 11)\n",
            "tf.Tensor(\n",
            "[[  1  53  94 ...   0   0   0]\n",
            " [  1   8  75 ...   0   0   0]\n",
            " [  1   6 132 ...   0   0   0]\n",
            " ...\n",
            " [  1   6 232 ...   0   0   0]\n",
            " [  1 171 303 ...   0   0   0]\n",
            " [  1   9 230 ...   0   0   0]], shape=(64, 16), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[   1    4   47  187  162    3    2    0    0    0    0]\n",
            " [   1   16   23   34   33  158    3    2    0    0    0]\n",
            " [   1   29    6  362   21  395    7    2    0    0    0]\n",
            " [   1   14  105  449    3    2    0    0    0    0    0]\n",
            " [   1    6   25   34  262   10    3    2    0    0    0]\n",
            " [   1    4   18  217   54    5    3    2    0    0    0]\n",
            " [   1   10   11    5   11  433    3    2    0    0    0]\n",
            " [   1  254  300   24  198    3    2    0    0    0    0]\n",
            " [   1  120  103    3    2    0    0    0    0    0    0]\n",
            " [   1    6   29   15  122    5    3    2    0    0    0]\n",
            " [   1   10   26   85 1759    3    2    0    0    0    0]\n",
            " [   1  316  203    3    2    0    0    0    0    0    0]\n",
            " [   1  149   15    5    3    2    0    0    0    0    0]\n",
            " [   1    4  236    3    2    0    0    0    0    0    0]\n",
            " [   1   13  755   92  225    3    2    0    0    0    0]\n",
            " [   1    4 1074  178    3    2    0    0    0    0    0]\n",
            " [   1   27 1147  109    3    2    0    0    0    0    0]\n",
            " [   1   14   11  547 2961    3    2    0    0    0    0]\n",
            " [   1    4   35   19 2273    3    2    0    0    0    0]\n",
            " [   1  388  413    8   19    7    2    0    0    0    0]\n",
            " [   1    5  540  140 1268    3    2    0    0    0    0]\n",
            " [   1   16   23  103  404    3    2    0    0    0    0]\n",
            " [   1   10   11  510   69    3    2    0    0    0    0]\n",
            " [   1    5  176   12  135  109    3    2    0    0    0]\n",
            " [   1    5  305   57    3    2    0    0    0    0    0]\n",
            " [   1   25    6   86 1223    7    2    0    0    0    0]\n",
            " [   1   27 2643   41    3    2    0    0    0    0    0]\n",
            " [   1  120  170  144   17    3    2    0    0    0    0]\n",
            " [   1    4  117    9  728    3    2    0    0    0    0]\n",
            " [   1    4  105   13  177    3    2    0    0    0    0]\n",
            " [   1   10   11   78  167   58    3    2    0    0    0]\n",
            " [   1    8   10  210  185    7    2    0    0    0    0]\n",
            " [   1    5   26    9  141  563    3    2    0    0    0]\n",
            " [   1   27   51  335   55   89    3    2    0    0    0]\n",
            " [   1  235   26  214  110    3    2    0    0    0    0]\n",
            " [   1  486   25  174   10    3    2    0    0    0    0]\n",
            " [   1   27  276   67  437    3    2    0    0    0    0]\n",
            " [   1   73   17   74    3    2    0    0    0    0    0]\n",
            " [   1   21  858  192    3    2    0    0    0    0    0]\n",
            " [   1  587 1841   44 1673    3    2    0    0    0    0]\n",
            " [   1    5 2427    3    2    0    0    0    0    0    0]\n",
            " [   1    5  274   15   40  326    3    2    0    0    0]\n",
            " [   1   60 1563    7    2    0    0    0    0    0    0]\n",
            " [   1   16   38  212   40   39    3    2    0    0    0]\n",
            " [   1    4 3172    6   15  102    3    2    0    0    0]\n",
            " [   1   52  635  189    8   37    2    0    0    0    0]\n",
            " [   1    4   30   12   35 2075    3    2    0    0    0]\n",
            " [   1   10  576  824    3    2    0    0    0    0    0]\n",
            " [   1   28   23  488    3    2    0    0    0    0    0]\n",
            " [   1    4   18  332  396   58    3    2    0    0    0]\n",
            " [   1   19  196   49 4216   37    2    0    0    0    0]\n",
            " [   1   32   25    6   98   17    7    2    0    0    0]\n",
            " [   1    8   20   31  312    7    2    0    0    0    0]\n",
            " [   1   32   22 1568  112    7    2    0    0    0    0]\n",
            " [   1    5  250   45 1214    3    2    0    0    0    0]\n",
            " [   1   14   11    9  463    3    2    0    0    0    0]\n",
            " [   1   25    4   36  314    7    2    0    0    0    0]\n",
            " [   1    4   77 1365    3    2    0    0    0    0    0]\n",
            " [   1   71   42    5   36    7    2    0    0    0    0]\n",
            " [   1    4  173    5  277  142    3    2    0    0    0]\n",
            " [   1    5    8  269   59   45    3    2    0    0    0]\n",
            " [   1  124   10  192    7    2    0    0    0    0    0]\n",
            " [   1    4  123   36   89  284    3    2    0    0    0]\n",
            " [   1  277   11    9 1143    3    2    0    0    0    0]], shape=(64, 11), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWAZqZy289ly"
      },
      "source": [
        "# 超参数定义\n",
        "\n",
        "# 将单词进行编码，编码长度为256\n",
        "embedding_units = 256   \n",
        "# 中间循环神经网络 encoder decoder  \n",
        "units = 1024          \n",
        "# 输入词表长度\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "# 输出词表长度\n",
        "output_vocab_size = len(output_tokenizer.word_index) + 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_BB8LDp50XO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73c09a76-9201-4ee2-d41f-1022e2027c66"
      },
      "source": [
        "# 调用子类API\n",
        "class Encoder(tf.keras.Model):\n",
        "  # 初始化函数\n",
        "  def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
        "    # 调用父类初始化函数\n",
        "    super(Encoder, self).__init__()\n",
        "    # 赋初值\n",
        "    self.batch_size = batch_size;\n",
        "    self.encoding_units = encoding_units;\n",
        "    # 一个规定输入词表大小和输出编码大小的编码器\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
        "    # 每一步的隐层状态： 矩阵 hide_state 最后一步的输出：cell_state\n",
        "    self.gru = keras.layers.GRU(self.encoding_units, return_sequences = True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    # 输入编码\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    # 每一步输出和最后一次输出的隐含状态\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):    \n",
        "    return tf.zeros((self.batch_size, self.encoding_units))\n",
        "\n",
        "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder.call(x, sample_hidden)\n",
        "print('output_shape')\n",
        "print(sample_output.shape)\n",
        "print('decoder_hidden_shape')\n",
        "print(sample_hidden.shape)\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output_shape\n",
            "(64, 16, 1024)\n",
            "decoder_hidden_shape\n",
            "(64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InQe9MmLXvGB",
        "outputId": "8f203f88-54d1-4bac-bc7c-9869fa6bdfab"
      },
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "  def __init__(self, units):\n",
        "    # units 全连接层的维度 y = A * x\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = keras.layers.Dense(units)\n",
        "    self.W2 = keras.layers.Dense(units)\n",
        "    self.V = keras.layers.Dense(1)\n",
        "  def call(self, decoder_hidden, encoder_outputs):\n",
        "    # decoder_hidden.shape: (batch_size,units) (64,1024)\n",
        "    # encoder_outputs.shape: (batch_size, length, units) (64,16,1024)\n",
        "    # decoder_hidden_with_time_axis: (batch_size,1,units) (64,1,1024)\n",
        "    # 保证大的维度一致，就可以相加\n",
        "\n",
        "    decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
        "\n",
        "    # before V: (batch_size, length, units) 64 16 10\n",
        "    # after V socre: (batch_size, length, 1)  64 16 1\n",
        "    # self.W1(encoder_outputs) 64 16 10\n",
        "    # self.W2(decoder_hidden_with_time_axis) 64 1 10\n",
        "    # tanh不改变维度\n",
        "    score = self.V(\n",
        "        tf.nn.tanh(\n",
        "            self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # attention_weights.shape: (batch_size, length, 1)\n",
        "    # attention 只和 单词（length） 有关系, 所以只能在length上做softmax\n",
        "    attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "    \n",
        "    # context_vector.shape: (batch_size, length, units) 64 16 1024\n",
        "    # 维度不匹配的相乘(忽略batch_size 维度)\n",
        "    # [attention_weight 按列复制1024份] * [encoder_outputs] 16*1024\n",
        "    # attention_weight.shape() 64 16 1; encoder_outputs.shape() 64 16 1024\n",
        "    # attention_weights 实际是length的权重\n",
        "    context_vector = attention_weights * encoder_outputs\n",
        "    \n",
        "    # context_vector.shape: (batch_size, units) 64 1024\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_model = BahdanauAttention(units = 10)\n",
        "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
        "\n",
        "print(\"attention_results.shape:\", attention_results.shape)\n",
        "print(\"attention_weights.shape:\", attention_weights.shape)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention_results.shape: (64, 1024)\n",
            "attention_weights.shape: (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}