{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq_seq_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bJC27ibhXVGWPSvgXOfdrdIvfyjx6W_8",
      "authorship_tag": "ABX9TyOfVtHXIw7YJek9YTNV9iH2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunshyPiKaChew/seq2seq_attention/blob/master/seq_seq_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnWPdM__tQJc",
        "outputId": "53dd1c7a-a5e8-49b4-b70d-e41156bdc4a3"
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==2.0.0-beta1 in /usr/local/lib/python3.7/dist-packages (2.0.0b1)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0a20190603)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.19.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.1.2)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.14.0.dev2019060501)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.12.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.2.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (3.12.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.3.3)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.0.0-beta1) (1.32.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (56.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.10.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMzpjEWgtQXz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ebb4a5-d059-4080-c4b9-8bac5b842577"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLixUxKutcl2",
        "outputId": "3c250b64-b281-4cbf-913b-e53bb0d91b80"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "  print(module.__name__, module.__version__)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0-beta1\n",
            "sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.2\n",
            "numpy 1.19.5\n",
            "pandas 1.1.5\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.0.0-beta1\n",
            "tensorflow.python.keras.api._v2.keras 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVyjESk2tzDv"
      },
      "source": [
        "# 1. preprocessing data\n",
        "# 2. build model\n",
        "# 2.1 encoder\n",
        "# 2.2 attetion\n",
        "# 2.3 decoder\n",
        "# 3. evaluation\n",
        "# 3.1 given sentence. return translate results\n",
        "# 3.2 visualize results (attention)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6hEAeCUvzlo",
        "outputId": "caa5a978-c3dc-41cb-f92a-23f3fb285157"
      },
      "source": [
        "# unicode2ascii 去掉西班牙语的重音\n",
        "import unicodedata\n",
        "def unicode_to_ascii(s):\n",
        "  # normalize 的 NFD 方法，如果一个unicode值包含多个字符，那么把他拆开，例如e和重音分开\n",
        "  # 'Mn' 重音的分类标志\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "en_sentence = 'Then what?'\n",
        "sp_sentence = '¿Entonces qué?'\n",
        "\n",
        "print(unicode_to_ascii(en_sentence))\n",
        "print(unicode_to_ascii(sp_sentence))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Then what?\n",
            "¿Entonces que?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSq4oDIhypbS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d8a8fa-1c52-4936-eb21-97a2f567dcf3"
      },
      "source": [
        "# 字符串预处理\n",
        "import re\n",
        "def preprocess_sentence(s):\n",
        "  s = unicode_to_ascii(s.lower().strip())\n",
        "  # [] 匹配操作 () 替换操作 前后加空格\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  # 将一个或多个空格替换为一个空格\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  # 除了字母和标点符号都替换为空格\n",
        "  s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
        "  # 去掉前后的空格\n",
        "  s = s.rstrip().strip()\n",
        "  # 添加前后特殊字符\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n",
        "\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> then what ? <end>\n",
            "<start> ¿ entonces que ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1orHnX7vNGD",
        "outputId": "c6bee836-62b6-4aa2-804c-761cd0fc17d5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "en_spa_file_path = '/content/drive/MyDrive/Colab/chapter_10/data_spa_en/spa.txt'"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q1aGPzXjdGQ",
        "outputId": "5c8ec4df-f995-4eb6-b29e-7e9b8d19f291"
      },
      "source": [
        "def parse_data(filename):\n",
        "  # 根据回车分割数据中的每一行\n",
        "  lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
        "  # 根据制表符分割西班牙文和英文\n",
        "  sentence_pairs = [line.split('\\t') for line in lines]\n",
        "  preprocessed_sentence_pairs = [\n",
        "    (preprocess_sentence(en), preprocess_sentence(sp)) for en,sp in sentence_pairs]\n",
        "  return zip(*preprocessed_sentence_pairs)\n",
        "\n",
        "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
        "print(en_dataset[-1])\n",
        "print(sp_dataset[-1])\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF7iAng9yziM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8d410f-0139-4159-9d7b-fd9f42941c52"
      },
      "source": [
        "# 文本式数据转化为ID式数据\n",
        "def tokenizer(lang):\n",
        "  lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words = None, filters='', split=' ')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  # Padding\n",
        "  tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
        "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
        "\n",
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "max_length_input = max_length(input_tensor)\n",
        "max_length_output = max_length(output_tensor)\n",
        "print(max_length_input, max_length_output)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3at6ZzSc_qZs",
        "outputId": "3198645d-acb7-475a-dc72-7cdb28bd7a85"
      },
      "source": [
        "# 调用sklearn函数分割数据集\n",
        "from sklearn.model_selection import train_test_split\n",
        "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size = 0.2)\n",
        "len(input_train),len(input_eval), len(output_train), len(output_eval)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 6000, 24000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA5l5F6iApKQ",
        "outputId": "8619a1b3-191a-4e37-dc43-83f4e098c1f7"
      },
      "source": [
        "def convert(example, tokenizer):\n",
        "  for t in example:\n",
        "    if t != 0:\n",
        "      print('%d --> %s' % (t,tokenizer.index_word[t]))\n",
        "\n",
        "convert(input_train[0],input_tokenizer)\n",
        "print()\n",
        "convert(output_train[0],output_tokenizer)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 --> <start>\n",
            "6 --> ¿\n",
            "51 --> estas\n",
            "643 --> mirando\n",
            "5 --> ?\n",
            "2 --> <end>\n",
            "\n",
            "1 --> <start>\n",
            "24 --> are\n",
            "6 --> you\n",
            "612 --> looking\n",
            "7 --> ?\n",
            "2 --> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J32VumKI-L7N",
        "outputId": "de520c03-3b5e-4d1f-d5bc-7bd7fad06eb8"
      },
      "source": [
        "# 比如训练集有50000个样本，而我设定的batch_size是50，也就是说每50个样本才更新一次参数，那么也就意味着一个epoch里会提取1000次bach，\n",
        "# 这样才会把每个样本都提取了一遍，更新了1000次参数。\n",
        "\n",
        "# 这是一个epoch里做的，依次类推，我要设定2000个epoch意味着把这个过程重复2000次。也就是训练集里的每个样本都被提取了2000次。\n",
        "\n",
        "# 生成DataSet\n",
        "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(30000)\n",
        "  dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
        "  return dataset\n",
        "\n",
        "batch_size = 64;\n",
        "epochs = 20\n",
        "\n",
        "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
        "eval_dataset = make_dataset(input_eval, output_eval, batch_size, epochs, False)\n",
        "\n",
        "# 64 是一个bacth的大小 16 11 分别为输入输出padding之后的大小\n",
        "for x,y in train_dataset.take(1):\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  print(x)\n",
        "  print(y)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 16)\n",
            "(64, 11)\n",
            "tf.Tensor(\n",
            "[[  1  23 199 ...   0   0   0]\n",
            " [  1  33 262 ...   0   0   0]\n",
            " [  1   4  12 ...   0   0   0]\n",
            " ...\n",
            " [  1   6 157 ...   0   0   0]\n",
            " [  1  65  37 ...   0   0   0]\n",
            " [  1  12  40 ...   0   0   0]], shape=(64, 16), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[   1    4  175  400   50   37    2    0    0    0    0]\n",
            " [   1  327   24  296    3    2    0    0    0    0    0]\n",
            " [   1    5  121   17    3    2    0    0    0    0    0]\n",
            " [   1   28  266   35 1041    3    2    0    0    0    0]\n",
            " [   1  124   14   35  731    7    2    0    0    0    0]\n",
            " [   1  174   10  131   49   56    3    2    0    0    0]\n",
            " [   1   20   11   32   27  175    3    2    0    0    0]\n",
            " [   1   16   23  999   59   20    3    2    0    0    0]\n",
            " [   1    4   29  160   15 2324    3    2    0    0    0]\n",
            " [   1    4  105   21  177    3    2    0    0    0    0]\n",
            " [   1   61   83    8    9 2622    3    2    0    0    0]\n",
            " [   1   58  125   15  245    3    2    0    0    0    0]\n",
            " [   1  459   10   33 4445    3    2    0    0    0    0]\n",
            " [   1    4   18  670    6    2    0    0    0    0    0]\n",
            " [   1    5   26    9  141  563    3    2    0    0    0]\n",
            " [   1   13 1911    8   48  521    3    2    0    0    0]\n",
            " [   1   71   11   13  687    7    2    0    0    0    0]\n",
            " [   1   28   24  155    3    2    0    0    0    0    0]\n",
            " [   1   24    6   55   89    7    2    0    0    0    0]\n",
            " [   1  351    3    2    0    0    0    0    0    0    0]\n",
            " [   1   58   32   11  184    7    2    0    0    0    0]\n",
            " [   1    4  328    5  661   93    3    2    0    0    0]\n",
            " [   1  267   12    6  633    7    2    0    0    0    0]\n",
            " [   1   13 1871   24  932    3    2    0    0    0    0]\n",
            " [   1   14  311  142   61  427    3    2    0    0    0]\n",
            " [   1   13 1469    8 2193    3    2    0    0    0    0]\n",
            " [   1   13  770    8  767    3    2    0    0    0    0]\n",
            " [   1   27  236   54   13  201    3    2    0    0    0]\n",
            " [   1   22    6   35  370    7    2    0    0    0    0]\n",
            " [   1   14   99  231  325    3    2    0    0    0    0]\n",
            " [   1    6   42  308   49    5    3    2    0    0    0]\n",
            " [   1    4  222    6    9 1244    3    2    0    0    0]\n",
            " [   1   53   39    3    2    0    0    0    0    0    0]\n",
            " [   1    5  108   40  531    3    2    0    0    0    0]\n",
            " [   1    5   75  631    6    3    2    0    0    0    0]\n",
            " [   1   22    6  262  158    7    2    0    0    0    0]\n",
            " [   1   25    6   36  898    7    2    0    0    0    0]\n",
            " [   1    5  386   45   50    3    2    0    0    0    0]\n",
            " [   1    4   43    6   23  148    3    2    0    0    0]\n",
            " [   1    6   36  285    3    2    0    0    0    0    0]\n",
            " [   1    5   87   12  136    9  407    3    2    0    0]\n",
            " [   1   56   53   33    3    2    0    0    0    0    0]\n",
            " [   1   30   12  450   21 1048    3    2    0    0    0]\n",
            " [   1   10   96  398 1486    3    2    0    0    0    0]\n",
            " [   1   68 2962   92  315    3    2    0    0    0    0]\n",
            " [   1   24    6  960    7    2    0    0    0    0    0]\n",
            " [   1   14  151   13  201  185    3    2    0    0    0]\n",
            " [   1   13 3601    8 1313    3    2    0    0    0    0]\n",
            " [   1    4  114    4   18  182    3    2    0    0    0]\n",
            " [   1    4  114   28   43   81    3    2    0    0    0]\n",
            " [   1   57   11   66  129   91    3    2    0    0    0]\n",
            " [   1   46   11   29  140  214    3    2    0    0    0]\n",
            " [   1   20   11  172  219  109    3    2    0    0    0]\n",
            " [   1    6  267   12  273    3    2    0    0    0    0]\n",
            " [   1  188  401    6    3    2    0    0    0    0    0]\n",
            " [   1  600  128    3    2    0    0    0    0    0    0]\n",
            " [   1  111   31  344   50    3    2    0    0    0    0]\n",
            " [   1    4   26   70  786    3    2    0    0    0    0]\n",
            " [   1    5 1470   61  449    3    2    0    0    0    0]\n",
            " [   1   10   26  223   69    3    2    0    0    0    0]\n",
            " [   1   14   11  269   59  403    3    2    0    0    0]\n",
            " [   1  397  730    8  263    7    2    0    0    0    0]\n",
            " [   1    4   29  945   84  290    3    2    0    0    0]\n",
            " [   1    4   35   68   48  154    3    2    0    0    0]], shape=(64, 11), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWAZqZy289ly"
      },
      "source": [
        "# 超参数定义\n",
        "\n",
        "# 将单词进行编码，编码长度为256\n",
        "embedding_units = 256   \n",
        "# 中间循环神经网络 encoder decoder  \n",
        "units = 1024          \n",
        "# 输入词表长度\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "# 输出词表长度\n",
        "output_vocab_size = len(output_tokenizer.word_index) + 1"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_BB8LDp50XO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55fd5fcc-83b9-4992-a375-9a6df7711eba"
      },
      "source": [
        "# 调用子类API\n",
        "class Encoder(tf.keras.Model):\n",
        "  # 初始化函数\n",
        "  def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
        "    # 调用父类初始化函数\n",
        "    super(Encoder, self).__init__()\n",
        "    # 赋初值\n",
        "    self.batch_size = batch_size;\n",
        "    self.encoding_units = encoding_units;\n",
        "    # 一个规定输入词表大小和输出编码大小的编码器\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
        "    # 每一步的隐层状态： 矩阵 hide_state 最后一步的输出：cell_state\n",
        "    self.gru = keras.layers.GRU(self.encoding_units, return_sequences = True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    # 输入编码\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    # 每一步输出和最后一次输出的隐含状态\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):    \n",
        "    return tf.zeros((self.batch_size, self.encoding_units))\n",
        "\n",
        "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder.call(x, sample_hidden)\n",
        "print('output_shape')\n",
        "print(sample_output.shape)\n",
        "print('decoder_hidden_shape')\n",
        "print(sample_hidden.shape)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output_shape\n",
            "(64, 16, 1024)\n",
            "decoder_hidden_shape\n",
            "(64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InQe9MmLXvGB",
        "outputId": "c5913911-4944-4068-8c64-d44d40d74647"
      },
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "  def __init__(self, units):\n",
        "    # units 全连接层的维度 y = A * x\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = keras.layers.Dense(units)\n",
        "    self.W2 = keras.layers.Dense(units)\n",
        "    self.V = keras.layers.Dense(1)\n",
        "  def call(self, decoder_hidden, encoder_outputs):\n",
        "    # decoder_hidden.shape: (batch_size,units) (64,1024)\n",
        "    # encoder_outputs.shape: (batch_size, length, units) (64,16,1024)\n",
        "    # decoder_hidden_with_time_axis: (batch_size,1,units) (64,1,1024)\n",
        "    # 保证大的维度一致，就可以相加\n",
        "\n",
        "    decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
        "\n",
        "    # before V: (batch_size, length, units) 64 16 10\n",
        "    # after V socre: (batch_size, length, 1)  64 16 1\n",
        "    # self.W1(encoder_outputs) 64 16 10\n",
        "    # self.W2(decoder_hidden_with_time_axis) 64 1 10\n",
        "    # tanh不改变维度\n",
        "    score = self.V(\n",
        "        tf.nn.tanh(\n",
        "            self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # attention_weights.shape: (batch_size, length, 1)\n",
        "    # attention 只和 单词（length） 有关系, 所以只能在length上做softmax\n",
        "    attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "    \n",
        "    # context_vector.shape: (batch_size, length, units) 64 16 1024\n",
        "    # 维度不匹配的相乘(忽略batch_size 维度)\n",
        "    # [attention_weight 按列复制1024份] * [encoder_outputs] 16*1024\n",
        "    # attention_weight.shape() 64 16 1; encoder_outputs.shape() 64 16 1024\n",
        "    # attention_weights 实际是length的权重\n",
        "    context_vector = attention_weights * encoder_outputs\n",
        "    \n",
        "    # context_vector.shape: (batch_size, units) 64 1024\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_model = BahdanauAttention(units = 10)\n",
        "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
        "\n",
        "print(\"attention_results.shape:\", attention_results.shape)\n",
        "print(\"attention_weights.shape:\", attention_weights.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention_results.shape: (64, 1024)\n",
            "attention_weights.shape: (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWZJpQQ16SRs",
        "outputId": "b903efa3-9cde-4185-c534-5358e211736d"
      },
      "source": [
        "class Decoder(keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.decoding_units = decoding_units\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
        "    self.gru = keras.layers.GRU(self.decoding_units, return_sequences= True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc = keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(decoding_units)\n",
        "  \n",
        "  def call(self, x, hidden, encoding_outputs):\n",
        "    # context_vector.shape: (batch_size, units) 64 1024\n",
        "    context_vector, attention_weights = self.attention(hidden, encoding_outputs)\n",
        "    # before embedding: x.shape: (batch_size,1) 64 1 一步编码，x长度是1不是16\n",
        "    x = self.embedding(x)\n",
        "    # after embedding: x.shape: (batch_size, 1, embedding_units) 64, 1, 256\n",
        "    # 按照最后一个维度拼接\n",
        "    combined_x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
        "    # output.shape: (batch_size, 1, decoding_units) 64 1 1024\n",
        "    # state.shape: (batch_size, decoding_units) 64 1024\n",
        "    output, state = self.gru(combined_x)\n",
        "\n",
        "    # output.shape: (batch_size, decoding_units)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output.shape： (batch_size, output_vocab_size)\n",
        "    output = self.fc(output)\n",
        "\n",
        "    return output, state, attention_weights\n",
        "\n",
        "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
        "outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
        "decoder_output, decoder_hidden, decoder_aw = outputs\n",
        "print(\"decoder_output.shape: \", decoder_output.shape)\n",
        "print(\"decoder_hidden.shape: \", decoder_hidden.shape)\n",
        "# attention 用的是sample数据，所以和16有关系\n",
        "print(\"decoder_attention_weights.shape: \", decoder_aw.shape)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder_output.shape:  (64, 4935)\n",
            "decoder_hidden.shape:  (64, 1024)\n",
            "decoder_attention_weights.shape:  (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx1H_qxqQA1l"
      },
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "# from_logits: True->没经过softmax False->经过softmax\n",
        "# reduction : mask 之后聚合\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # padding部分为0，非padding部分为1\n",
        "  mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  # 精度转化\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crCz78WPTSny"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}