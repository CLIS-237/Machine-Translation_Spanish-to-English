{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq_seq_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bJC27ibhXVGWPSvgXOfdrdIvfyjx6W_8",
      "authorship_tag": "ABX9TyOCbxyZdzoOAVsWVI6cchsb",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunshyPiKaChew/seq2seq_attention/blob/master/seq_seq_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnWPdM__tQJc"
      },
      "source": [
        "#!pip install tensorflow==2.0.0-beta1\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMzpjEWgtQXz",
        "outputId": "d1cd9e75-d198-4888-d009-04759ec7b793"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLixUxKutcl2",
        "outputId": "50402e84-307d-44d2-d27f-edce9f074b59"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "  print(module.__name__, module.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.2\n",
            "numpy 1.19.5\n",
            "pandas 1.1.5\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.4.1\n",
            "tensorflow.keras 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVyjESk2tzDv"
      },
      "source": [
        "# 1. preprocessing data\n",
        "# 2. build model\n",
        "# 2.1 encoder\n",
        "# 2.2 attetion\n",
        "# 2.3 decoder\n",
        "# 3. evaluation\n",
        "# 3.1 given sentence. return translate results\n",
        "# 3.2 visualize results (attention)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6hEAeCUvzlo",
        "outputId": "4025c0f9-5fdc-4f38-9ddd-6b02f18bf293"
      },
      "source": [
        "# unicode2ascii 去掉西班牙语的重音\n",
        "import unicodedata\n",
        "def unicode_to_ascii(s):\n",
        "  # normalize 的 NFD 方法，如果一个unicode值包含多个字符，那么把他拆开，例如e和重音分开\n",
        "  # 'Mn' 重音的分类标志\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "en_sentence = 'Then what?'\n",
        "sp_sentence = '¿Entonces qué?'\n",
        "\n",
        "print(unicode_to_ascii(en_sentence))\n",
        "print(unicode_to_ascii(sp_sentence))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Then what?\n",
            "¿Entonces que?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSq4oDIhypbS",
        "outputId": "093cc0a6-4154-43c6-e232-3c0cbca33420"
      },
      "source": [
        "# 字符串预处理\n",
        "import re\n",
        "def preprocess_sentence(s):\n",
        "  s = unicode_to_ascii(s.lower().strip())\n",
        "  # [] 匹配操作 () 替换操作 前后加空格\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  # 将一个或多个空格替换为一个空格\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  # 除了字母和标点符号都替换为空格\n",
        "  s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
        "  # 去掉前后的空格\n",
        "  s = s.rstrip().strip()\n",
        "  # 添加前后特殊字符\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n",
        "\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> then what ? <end>\n",
            "<start> ¿ entonces que ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1orHnX7vNGD",
        "outputId": "4df092a3-f05f-40b7-922e-9379f68239f2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "en_spa_file_path = '/content/drive/MyDrive/Colab/chapter_10/data_spa_en/spa.txt'\n",
        "\n",
        "# 非挂载方式\n",
        "#en_spa_file_path = '/content/train_local/spa.txt'\n",
        "\n",
        "# 尝试2\n",
        "#!mkdir ./data\n",
        "#!cp ./drive/MyDrive/Colab\\ Notebooks/spa.zip ./data/\n",
        "#!cd ./data && unzip spa.zip\n",
        "#en_spa_file_path = './data/spa.txt'"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q1aGPzXjdGQ",
        "outputId": "d92226b3-2d2a-464e-d982-0c66da74b5cb"
      },
      "source": [
        "def parse_data(filename):\n",
        "  # 根据回车分割数据中的每一行\n",
        "  lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
        "  # 根据制表符分割西班牙文和英文\n",
        "  sentence_pairs = [line.split('\\t') for line in lines]\n",
        "  preprocessed_sentence_pairs = [\n",
        "    (preprocess_sentence(en), preprocess_sentence(sp)) for en,sp in sentence_pairs]\n",
        "  return zip(*preprocessed_sentence_pairs)\n",
        "\n",
        "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
        "print(en_dataset[-1])\n",
        "print(sp_dataset[-1])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF7iAng9yziM",
        "outputId": "3147e759-96cd-4691-af86-e0d9a9fd1a08"
      },
      "source": [
        "# 文本式数据转化为ID式数据\n",
        "def tokenizer(lang):\n",
        "  lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words = None, filters='', split=' ')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  # Padding\n",
        "  tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
        "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
        "\n",
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "max_length_input = max_length(input_tensor)\n",
        "max_length_output = max_length(output_tensor)\n",
        "print(max_length_input, max_length_output)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3at6ZzSc_qZs",
        "outputId": "9813d97f-08f3-4d60-8ff8-d5c97fa2e6a8"
      },
      "source": [
        "# 调用sklearn函数分割数据集\n",
        "from sklearn.model_selection import train_test_split\n",
        "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size = 0.2)\n",
        "len(input_train),len(input_eval), len(output_train), len(output_eval)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 6000, 24000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA5l5F6iApKQ",
        "outputId": "ec4c5888-eee4-46f3-b995-a4d83d1c0ab3"
      },
      "source": [
        "def convert(example, tokenizer):\n",
        "  for t in example:\n",
        "    if t != 0:\n",
        "      print('%d --> %s' % (t,tokenizer.index_word[t]))\n",
        "\n",
        "convert(input_train[0],input_tokenizer)\n",
        "print()\n",
        "convert(output_train[0],output_tokenizer)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 --> <start>\n",
            "1095 --> admiro\n",
            "10 --> a\n",
            "4 --> tom\n",
            "3 --> .\n",
            "2 --> <end>\n",
            "\n",
            "1 --> <start>\n",
            "4 --> i\n",
            "1024 --> admire\n",
            "5 --> tom\n",
            "3 --> .\n",
            "2 --> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J32VumKI-L7N",
        "outputId": "33734549-78a8-4a72-e0a2-e6b8388a0da2"
      },
      "source": [
        "# 比如训练集有50000个样本，而我设定的batch_size是50，也就是说每50个样本才更新一次参数，那么也就意味着一个epoch里会提取1000次bach，\n",
        "# 这样才会把每个样本都提取了一遍，更新了1000次参数。\n",
        "\n",
        "# 这是一个epoch里做的，依次类推，我要设定2000个epoch意味着把这个过程重复2000次。也就是训练集里的每个样本都被提取了2000次。\n",
        "\n",
        "# 生成DataSet\n",
        "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(30000)\n",
        "  dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
        "  return dataset\n",
        "\n",
        "batch_size = 64;\n",
        "epochs = 20\n",
        "\n",
        "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
        "eval_dataset = make_dataset(input_eval, output_eval, batch_size, epochs, False)\n",
        "\n",
        "# 64 是一个bacth的大小 16 11 分别为输入输出padding之后的大小\n",
        "for x,y in train_dataset.take(1):\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  print(x)\n",
        "  print(y)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 16)\n",
            "(64, 11)\n",
            "tf.Tensor(\n",
            "[[   1   30   55 ...    0    0    0]\n",
            " [   1    6  137 ...    0    0    0]\n",
            " [   1    6 1656 ...    0    0    0]\n",
            " ...\n",
            " [   1   13  366 ...    0    0    0]\n",
            " [   1   52   36 ...    0    0    0]\n",
            " [   1  352  849 ...    0    0    0]], shape=(64, 16), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[   1   19   26    9  506    3    2    0    0    0    0]\n",
            " [   1  486  192    7    2    0    0    0    0    0    0]\n",
            " [   1   22    6  207 1916    7    2    0    0    0    0]\n",
            " [   1    4   35  327    3    2    0    0    0    0    0]\n",
            " [   1    4   63  162 3080    3    2    0    0    0    0]\n",
            " [   1   25 1593  246    7    2    0    0    0    0    0]\n",
            " [   1   16   65  160  338    3    2    0    0    0    0]\n",
            " [   1   20   11   21  310    3    2    0    0    0    0]\n",
            " [   1   10  678   17  691    3    2    0    0    0    0]\n",
            " [   1   14  511  295  109    3    2    0    0    0    0]\n",
            " [   1    4  161 1273 1216    3    2    0    0    0    0]\n",
            " [   1  174   13 2070    3    2    0    0    0    0    0]\n",
            " [   1    4  100   13 1351 1298    3    2    0    0    0]\n",
            " [   1   21 2998   76  105    3    2    0    0    0    0]\n",
            " [   1    6   23  955    3    2    0    0    0    0    0]\n",
            " [   1   60  420    7    2    0    0    0    0    0    0]\n",
            " [   1    4  234   60   14   26    3    2    0    0    0]\n",
            " [   1    4   63  162   73    3    2    0    0    0    0]\n",
            " [   1   46   11  171   41    3    2    0    0    0    0]\n",
            " [   1   14  629   54 1597    3    2    0    0    0    0]\n",
            " [   1    4  194  650   34  295    3    2    0    0    0]\n",
            " [   1    4   65  250  119    6    3    2    0    0    0]\n",
            " [   1   14   11    9 3539    3    2    0    0    0    0]\n",
            " [   1    6  663    3    2    0    0    0    0    0    0]\n",
            " [   1 2206 3682    3    2    0    0    0    0    0    0]\n",
            " [   1   32   22    6  221    7    2    0    0    0    0]\n",
            " [   1    5   38  202   74    3    2    0    0    0    0]\n",
            " [   1  111  857    3    2    0    0    0    0    0    0]\n",
            " [   1    4 2158  158    3    2    0    0    0    0    0]\n",
            " [   1    8    5  252    7    2    0    0    0    0    0]\n",
            " [   1   22    6   35 1856    7    2    0    0    0    0]\n",
            " [   1   28  145   33    3    2    0    0    0    0    0]\n",
            " [   1  397  356    8  263    7    2    0    0    0    0]\n",
            " [   1   14   51    9  405   83    3    2    0    0    0]\n",
            " [   1    4   29   84  418    3    2    0    0    0    0]\n",
            " [   1   70   64   91   39    3    2    0    0    0    0]\n",
            " [   1    5 1684    3    2    0    0    0    0    0    0]\n",
            " [   1    6  313   48  224    3    2    0    0    0    0]\n",
            " [   1    5  123   64    9  134    3    2    0    0    0]\n",
            " [   1   24  254  263    7    2    0    0    0    0    0]\n",
            " [   1    4   43   10   11 1144    3    2    0    0    0]\n",
            " [   1  111   33  450    3    2    0    0    0    0    0]\n",
            " [   1    4   43    9  817  462    3    2    0    0    0]\n",
            " [   1  497   17    3    2    0    0    0    0    0    0]\n",
            " [   1  199   10   57    3    2    0    0    0    0    0]\n",
            " [   1   13 1058 3116  128    3    2    0    0    0    0]\n",
            " [   1   24    6  101   58    7    2    0    0    0    0]\n",
            " [   1  174   10  131    3    2    0    0    0    0    0]\n",
            " [   1   62    4 1472    6    7    2    0    0    0    0]\n",
            " [   1    4   18  691   49   78    3    2    0    0    0]\n",
            " [   1   25  442   73   17    7    2    0    0    0    0]\n",
            " [   1    6   92  492    3    2    0    0    0    0    0]\n",
            " [   1 1290   10   11   78  167    3    2    0    0    0]\n",
            " [   1   19    8   21  387    3    2    0    0    0    0]\n",
            " [   1    5  276  759    3    2    0    0    0    0    0]\n",
            " [   1   30   12  136  242    3    2    0    0    0    0]\n",
            " [   1    4  420    3    2    0    0    0    0    0    0]\n",
            " [   1    5 1546   33    3    2    0    0    0    0    0]\n",
            " [   1   22    6   35 2697    7    2    0    0    0    0]\n",
            " [   1    4   43   71   15  264    3    2    0    0    0]\n",
            " [   1   42    6  456   13  655    7    2    0    0    0]\n",
            " [   1   13  227    8   66    3    2    0    0    0    0]\n",
            " [   1    6   24   48  549    3    2    0    0    0    0]\n",
            " [   1   86  783   37    2    0    0    0    0    0    0]], shape=(64, 11), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWAZqZy289ly"
      },
      "source": [
        "# 超参数定义\n",
        "\n",
        "# 将单词进行编码，编码长度为256\n",
        "embedding_units = 256   \n",
        "# 中间循环神经网络 encoder decoder  \n",
        "units = 1024          \n",
        "# 输入词表长度\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "# 输出词表长度\n",
        "output_vocab_size = len(output_tokenizer.word_index) + 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_BB8LDp50XO",
        "outputId": "726b3eac-4bf7-482a-ae59-90285a5e53ca"
      },
      "source": [
        "# 调用子类API\n",
        "class Encoder(tf.keras.Model):\n",
        "  # 初始化函数\n",
        "  def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
        "    # 调用父类初始化函数\n",
        "    super(Encoder, self).__init__()\n",
        "    # 赋初值\n",
        "    self.batch_size = batch_size;\n",
        "    self.encoding_units = encoding_units;\n",
        "    # 一个规定输入词表大小和输出编码大小的编码器\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
        "    # 每一步的隐层状态： 矩阵 hide_state 最后一步的输出：cell_state\n",
        "    self.gru = keras.layers.GRU(self.encoding_units, return_sequences = True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    # 输入编码\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    # 每一步输出和最后一次输出的隐含状态\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):    \n",
        "    return tf.zeros((self.batch_size, self.encoding_units))\n",
        "\n",
        "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder.call(x, sample_hidden)\n",
        "print('output_shape')\n",
        "print(sample_output.shape)\n",
        "print('decoder_hidden_shape')\n",
        "print(sample_hidden.shape)\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output_shape\n",
            "(64, 16, 1024)\n",
            "decoder_hidden_shape\n",
            "(64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InQe9MmLXvGB",
        "outputId": "da94dc3a-80d9-4646-b9f6-36aa4bfcf4dc"
      },
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "  def __init__(self, units):\n",
        "    # units 全连接层的维度 y = A * x\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = keras.layers.Dense(units)\n",
        "    self.W2 = keras.layers.Dense(units)\n",
        "    self.V = keras.layers.Dense(1)\n",
        "  def call(self, decoder_hidden, encoder_outputs):\n",
        "    # decoder_hidden.shape: (batch_size,units) (64,1024)\n",
        "    # encoder_outputs.shape: (batch_size, length, units) (64,16,1024)\n",
        "    # decoder_hidden_with_time_axis: (batch_size,1,units) (64,1,1024)\n",
        "    # 保证大的维度一致，就可以相加\n",
        "\n",
        "    decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
        "\n",
        "    # before V: (batch_size, length, units) 64 16 10\n",
        "    # after V socre: (batch_size, length, 1)  64 16 1\n",
        "    # self.W1(encoder_outputs) 64 16 10\n",
        "    # self.W2(decoder_hidden_with_time_axis) 64 1 10\n",
        "    # tanh不改变维度\n",
        "    score = self.V(\n",
        "        tf.nn.tanh(\n",
        "            self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # attention_weights.shape: (batch_size, length, 1)\n",
        "    # attention 只和 单词（length） 有关系, 所以只能在length上做softmax\n",
        "    attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "    \n",
        "    # context_vector.shape: (batch_size, length, units) 64 16 1024\n",
        "    # 维度不匹配的相乘(忽略batch_size 维度)\n",
        "    # [attention_weight 按列复制1024份] * [encoder_outputs] 16*1024\n",
        "    # attention_weight.shape() 64 16 1; encoder_outputs.shape() 64 16 1024\n",
        "    # attention_weights 实际是length的权重\n",
        "    context_vector = attention_weights * encoder_outputs\n",
        "    \n",
        "    # context_vector.shape: (batch_size, units) 64 1024\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_model = BahdanauAttention(units = 10)\n",
        "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
        "\n",
        "print(\"attention_results.shape:\", attention_results.shape)\n",
        "print(\"attention_weights.shape:\", attention_weights.shape)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention_results.shape: (64, 1024)\n",
            "attention_weights.shape: (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWZJpQQ16SRs",
        "outputId": "aede7069-0e76-4a39-e0b5-283af7b1fef0"
      },
      "source": [
        "class Decoder(keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.decoding_units = decoding_units\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
        "    self.gru = keras.layers.GRU(self.decoding_units, return_sequences= True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc = keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(decoding_units)\n",
        "  \n",
        "  def call(self, x, hidden, encoding_outputs):\n",
        "    # context_vector.shape: (batch_size, units) 64 1024\n",
        "    context_vector, attention_weights = self.attention(hidden, encoding_outputs)\n",
        "    # before embedding: x.shape: (batch_size,1) 64 1 一步编码，x长度是1不是16\n",
        "    x = self.embedding(x)\n",
        "    # after embedding: x.shape: (batch_size, 1, embedding_units) 64, 1, 256\n",
        "    # 按照最后一个维度拼接\n",
        "    combined_x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
        "    # output.shape: (batch_size, 1, decoding_units) 64 1 1024\n",
        "    # state.shape: (batch_size, decoding_units) 64 1024\n",
        "    output, state = self.gru(combined_x)\n",
        "\n",
        "    # output.shape: (batch_size, decoding_units)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output.shape： (batch_size, output_vocab_size)\n",
        "    output = self.fc(output)\n",
        "\n",
        "    return output, state, attention_weights\n",
        "\n",
        "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
        "outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
        "decoder_output, decoder_hidden, decoder_aw = outputs\n",
        "print(\"decoder_output.shape: \", decoder_output.shape)\n",
        "print(\"decoder_hidden.shape: \", decoder_hidden.shape)\n",
        "# attention 用的是sample数据，所以和16有关系\n",
        "print(\"decoder_attention_weights.shape: \", decoder_aw.shape)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder_output.shape:  (64, 4935)\n",
            "decoder_hidden.shape:  (64, 1024)\n",
            "decoder_attention_weights.shape:  (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx1H_qxqQA1l"
      },
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "# from_logits: True->没经过softmax False->经过softmax\n",
        "# reduction : mask 之后聚合\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # padding部分为0，非padding部分为1\n",
        "  mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  # 精度转化\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crCz78WPTSny"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, encoding_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    encoding_output, encoding_hidden = encoder(inp, encoding_hidden)\n",
        "    decoding_hidden = encoding_hidden\n",
        "    # eg:<start> I am here <end>\n",
        "    # 1. <start> -> I\n",
        "    # 2. I -> am\n",
        "    # 3. am -> here\n",
        "    # 4. here -> <end>\n",
        "    for t in range(0, targ.shape[1] - 1):\n",
        "      # batch_size 1\n",
        "      decoding_input = tf.expand_dims(targ[:,t], 1)\n",
        "      prediction, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_output)\n",
        "      loss += loss_function(targ[:,t+1], prediction)\n",
        "    \n",
        "  batch_loss = loss/int(targ.shape[0])\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pSyjvG7RQnd",
        "outputId": "b7085c06-ccea-4d90-c659-aec705c7677d"
      },
      "source": [
        "epochs = 10\n",
        "step_per_epoch = len(input_tensor) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  start = time.time()\n",
        "  encoding_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # enumerate的返回形式 (batch, (inp, targ))\n",
        "  for(batch, (inp, targ)) in enumerate(train_dataset.take(step_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, encoding_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / step_per_epoch))\n",
        "  print('Time take for 1 epoch {} sec\\n'.format(time.time()-start))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.7931\n",
            "Epoch 1 Batch 100 Loss 0.3765\n",
            "Epoch 1 Batch 200 Loss 0.3205\n",
            "Epoch 1 Batch 300 Loss 0.2840\n",
            "Epoch 1 Batch 400 Loss 0.2504\n",
            "Epoch 1 Loss 0.3333\n",
            "Time take for 1 epoch 1747.8085074424744 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2366\n",
            "Epoch 2 Batch 100 Loss 0.2182\n",
            "Epoch 2 Batch 200 Loss 0.2294\n",
            "Epoch 2 Batch 300 Loss 0.2037\n",
            "Epoch 2 Batch 400 Loss 0.1675\n",
            "Epoch 2 Loss 0.2087\n",
            "Time take for 1 epoch 1783.8532795906067 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.1424\n",
            "Epoch 3 Batch 100 Loss 0.1524\n",
            "Epoch 3 Batch 200 Loss 0.1617\n",
            "Epoch 3 Batch 300 Loss 0.1556\n",
            "Epoch 3 Batch 400 Loss 0.0953\n",
            "Epoch 3 Loss 0.1297\n",
            "Time take for 1 epoch 1806.8164820671082 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.0954\n",
            "Epoch 4 Batch 100 Loss 0.0852\n",
            "Epoch 4 Batch 200 Loss 0.0774\n",
            "Epoch 4 Batch 300 Loss 0.0879\n",
            "Epoch 4 Batch 400 Loss 0.0536\n",
            "Epoch 4 Loss 0.0796\n",
            "Time take for 1 epoch 1713.6320838928223 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.0408\n",
            "Epoch 5 Batch 100 Loss 0.0464\n",
            "Epoch 5 Batch 200 Loss 0.0495\n",
            "Epoch 5 Batch 300 Loss 0.0601\n",
            "Epoch 5 Batch 400 Loss 0.0228\n",
            "Epoch 5 Loss 0.0505\n",
            "Time take for 1 epoch 1714.358787536621 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.0258\n",
            "Epoch 6 Batch 100 Loss 0.0366\n",
            "Epoch 6 Batch 200 Loss 0.0504\n",
            "Epoch 6 Batch 300 Loss 0.0249\n",
            "Epoch 6 Batch 400 Loss 0.0207\n",
            "Epoch 6 Loss 0.0339\n",
            "Time take for 1 epoch 1713.2260019779205 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0194\n",
            "Epoch 7 Batch 100 Loss 0.0195\n",
            "Epoch 7 Batch 200 Loss 0.0295\n",
            "Epoch 7 Batch 300 Loss 0.0212\n",
            "Epoch 7 Batch 400 Loss 0.0194\n",
            "Epoch 7 Loss 0.0238\n",
            "Time take for 1 epoch 1700.1806745529175 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0138\n",
            "Epoch 8 Batch 100 Loss 0.0174\n",
            "Epoch 8 Batch 200 Loss 0.0181\n",
            "Epoch 8 Batch 300 Loss 0.0209\n",
            "Epoch 8 Batch 400 Loss 0.0119\n",
            "Epoch 8 Loss 0.0180\n",
            "Time take for 1 epoch 1692.353857755661 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0143\n",
            "Epoch 9 Batch 100 Loss 0.0100\n",
            "Epoch 9 Batch 200 Loss 0.0097\n",
            "Epoch 9 Batch 300 Loss 0.0117\n",
            "Epoch 9 Batch 400 Loss 0.0089\n",
            "Epoch 9 Loss 0.0149\n",
            "Time take for 1 epoch 1715.764030456543 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0062\n",
            "Epoch 10 Batch 100 Loss 0.0077\n",
            "Epoch 10 Batch 200 Loss 0.0154\n",
            "Epoch 10 Batch 300 Loss 0.0173\n",
            "Epoch 10 Batch 400 Loss 0.0122\n",
            "Epoch 10 Loss 0.0127\n",
            "Time take for 1 epoch 1696.479917049408 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lMwkmq5BYys"
      },
      "source": [
        "def evalute(input_sentence):\n",
        "  attention_matrix = np.zeros((max_length_output, max_length_input))\n",
        "  input_sentence = preprocess_sentence(input_sentence)\n",
        "\n",
        "  inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n",
        "  inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen = max_length_input, padding = 'post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  results = ''\n",
        "  encoding_hidden = tf.zeros((1,units))\n",
        "\n",
        "  encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
        "  decoding_hidden = encoding_hidden\n",
        "\n",
        "  # eg: <start> -> A -> B -> ...\n",
        "  # decoding_input.shape (1,1) 为batch扩展维度\n",
        "  decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']],0)\n",
        "  \n",
        "  for t in range(max_length_output):\n",
        "    predictions, decoding_hidden, attention_weights = decoder(\n",
        "        decoding_input, decoding_hidden, encoding_outputs)\n",
        "    # attention_weights.shape: (batch_size, input_length, 1) (1,16,1)\n",
        "    attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "    # attention_weights.shape: 1 16\n",
        "    attention_matrix[t] = attention_weights.numpy()\n",
        "\n",
        "    # predictions.shape: [batch_size, vocab_size] (1,4935)\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    results += output_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if output_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return results, input_sentence, attention_matrix\n",
        "    # decoding_input.shape (1,1) 为batch扩展维度\n",
        "    decoding_input = tf.expand_dims([predicted_id],0)\n",
        "  return results, input_sentence, attention_matrix\n",
        "\n",
        "# 绘制attention_matrix\n",
        "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1,1,1)\n",
        "  ax.matshow(attention_matrix, cmap = 'viridis')\n",
        "  font_dict = {'fontsize': 14}\n",
        "  ax.set_xticklabels([''] + input_sentence,\n",
        "              fontdict = font_dict, rotation = 90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence,\n",
        "              fontdict = font_dict,)\n",
        "  plt.show()\n",
        "  \n",
        "def translate(input_sentence):\n",
        "  results, input_sentence, attention_matrix = evalute(input_sentence)\n",
        "  print(\"Input: %s\" % (input_sentence))\n",
        "  print(\"Predicted translation: %s\" % (results))\n",
        "  # 去除padding部分\n",
        "  attention_matrix = attention_matrix[:len(results.split(' ')),\n",
        "                      :len(input_sentence.split(' '))]\n",
        "  plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "aY9VjIyKLcRP",
        "outputId": "30227d98-2b42-4da3-f6a1-b3559d372dda"
      },
      "source": [
        "# En : it's really cold here\n",
        "translate(u'hace mucho frío aquí.')"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: it s very cold here . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}