{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq_seq_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bJC27ibhXVGWPSvgXOfdrdIvfyjx6W_8",
      "authorship_tag": "ABX9TyM/BrgNiDsQqMq+Nr/YEcaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunshyPiKaChew/seq2seq_attention/blob/master/seq_seq_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnWPdM__tQJc"
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMzpjEWgtQXz"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLixUxKutcl2"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "  print(module.__name__, module.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVyjESk2tzDv"
      },
      "source": [
        "# 1. preprocessing data\n",
        "# 2. build model\n",
        "# 2.1 encoder\n",
        "# 2.2 attetion\n",
        "# 2.3 decoder\n",
        "# 3. evaluation\n",
        "# 3.1 given sentence. return translate results\n",
        "# 3.2 visualize results (attention)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6hEAeCUvzlo"
      },
      "source": [
        "# unicode2ascii 去掉西班牙语的重音\n",
        "import unicodedata\n",
        "def unicode_to_ascii(s):\n",
        "  # normalize 的 NFD 方法，如果一个unicode值包含多个字符，那么把他拆开，例如e和重音分开\n",
        "  # 'Mn' 重音的分类标志\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "en_sentence = 'Then what?'\n",
        "sp_sentence = '¿Entonces qué?'\n",
        "\n",
        "print(unicode_to_ascii(en_sentence))\n",
        "print(unicode_to_ascii(sp_sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSq4oDIhypbS"
      },
      "source": [
        "# 字符串预处理\n",
        "import re\n",
        "def preprocess_sentence(s):\n",
        "  s = unicode_to_ascii(s.lower().strip())\n",
        "  # [] 匹配操作 () 替换操作 前后加空格\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  # 将一个或多个空格替换为一个空格\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  # 除了字母和标点符号都替换为空格\n",
        "  s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
        "  # 去掉前后的空格\n",
        "  s = s.rstrip().strip()\n",
        "  # 添加前后特殊字符\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n",
        "\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1orHnX7vNGD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "en_spa_file_path = '/content/drive/MyDrive/Colab/chapter_10/data_spa_en/spa.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q1aGPzXjdGQ"
      },
      "source": [
        "def parse_data(filename):\n",
        "  # 根据回车分割数据中的每一行\n",
        "  lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
        "  # 根据制表符分割西班牙文和英文\n",
        "  sentence_pairs = [line.split('\\t') for line in lines]\n",
        "  preprocessed_sentence_pairs = [\n",
        "    (preprocess_sentence(en), preprocess_sentence(sp)) for en,sp in sentence_pairs]\n",
        "  return zip(*preprocessed_sentence_pairs)\n",
        "\n",
        "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
        "print(en_dataset[-1])\n",
        "print(sp_dataset[-1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF7iAng9yziM"
      },
      "source": [
        "# 文本式数据转化为ID式数据\n",
        "def tokenizer(lang):\n",
        "  lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words = None, filters='', split=' ')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  # Padding\n",
        "  tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
        "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
        "\n",
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "max_length_input = max_length(input_tensor)\n",
        "max_length_output = max_length(output_tensor)\n",
        "print(max_length_input, max_length_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3at6ZzSc_qZs"
      },
      "source": [
        "# 调用sklearn函数分割数据集\n",
        "from sklearn.model_selection import train_test_split\n",
        "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size = 0.2)\n",
        "len(input_train),len(input_eval), len(output_train), len(output_eval)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA5l5F6iApKQ"
      },
      "source": [
        "def convert(example, tokenizer):\n",
        "  for t in example:\n",
        "    if t != 0:\n",
        "      print('%d --> %s' % (t,tokenizer.index_word[t]))\n",
        "\n",
        "convert(input_train[0],input_tokenizer)\n",
        "print()\n",
        "convert(output_train[0],output_tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J32VumKI-L7N"
      },
      "source": [
        "# 比如训练集有50000个样本，而我设定的batch_size是50，也就是说每50个样本才更新一次参数，那么也就意味着一个epoch里会提取1000次bach，\n",
        "# 这样才会把每个样本都提取了一遍，更新了1000次参数。\n",
        "\n",
        "# 这是一个epoch里做的，依次类推，我要设定2000个epoch意味着把这个过程重复2000次。也就是训练集里的每个样本都被提取了2000次。\n",
        "\n",
        "# 生成DataSet\n",
        "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(30000)\n",
        "  dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
        "  return dataset\n",
        "\n",
        "batch_size = 64;\n",
        "epochs = 20\n",
        "\n",
        "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
        "eval_dataset = make_dataset(input_eval, output_eval, batch_size, epochs, False)\n",
        "\n",
        "# 64 是一个bacth的大小 16 11 分别为输入输出padding之后的大小\n",
        "for x,y in train_dataset.take(1):\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  print(x)\n",
        "  print(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWAZqZy289ly"
      },
      "source": [
        "# 超参数定义\n",
        "\n",
        "# 将单词进行编码，编码长度为256\n",
        "embedding_units = 256   \n",
        "# 中间循环神经网络 encoder decoder  \n",
        "units = 1024          \n",
        "# 输入词表长度\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "# 输出词表长度\n",
        "output_vocab_size = len(output_tokenizer.word_index) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_BB8LDp50XO"
      },
      "source": [
        "# 调用子类API\n",
        "class Encoder(tf.keras.Model):\n",
        "  # 初始化函数\n",
        "  def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
        "    # 调用父类初始化函数\n",
        "    super(Encoder, self).__init__()\n",
        "    # 赋初值\n",
        "    self.batch_size = batch_size;\n",
        "    self.encoding_units = encoding_units;\n",
        "    # 一个规定输入词表大小和输出编码大小的编码器\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
        "    # 每一步的隐层状态： 矩阵 hide_state 最后一步的输出：cell_state\n",
        "    self.gru = keras.layers.GRU(self.encoding_units, return_sequences = True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    # 输入编码\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    # 每一步输出和最后一次输出的隐含状态\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):    \n",
        "    return tf.zeros((self.batch_size, self.encoding_units))\n",
        "\n",
        "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder.call(x, sample_hidden)\n",
        "print('output_shape')\n",
        "print(sample_output.shape)\n",
        "print('decoder_hidden_shape')\n",
        "print(sample_hidden.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "InQe9MmLXvGB"
      },
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "  def __init__(self, units):\n",
        "    # units 全连接层的维度 y = A * x\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = keras.layers.Dense(units)\n",
        "    self.W2 = keras.layers.Dense(units)\n",
        "    self.V = keras.layers.Dense(1)\n",
        "  def call(self, decoder_hidden, encoder_outputs):\n",
        "    # decoder_hidden.shape: (batch_size,units) (64,1024)\n",
        "    # encoder_outputs.shape: (batch_size, length, units) (64,16,1024)\n",
        "    # decoder_hidden_with_time_axis: (batch_size,1,units) (64,1,1024)\n",
        "    # 保证大的维度一致，就可以相加\n",
        "\n",
        "    decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
        "\n",
        "    # before V: (batch_size, length, units) 64 16 10\n",
        "    # after V socre: (batch_size, length, 1)  64 16 1\n",
        "    # self.W1(encoder_outputs) 64 16 10\n",
        "    # self.W2(decoder_hidden_with_time_axis) 64 1 10\n",
        "    # tanh不改变维度\n",
        "    score = self.V(\n",
        "        tf.nn.tanh(\n",
        "            self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # attention_weights.shape: (batch_size, length, 1)\n",
        "    # attention 只和 单词（length） 有关系, 所以只能在length上做softmax\n",
        "    attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "    \n",
        "    # context_vector.shape: (batch_size, length, units) 64 16 1024\n",
        "    # 维度不匹配的相乘(忽略batch_size 维度)\n",
        "    # [attention_weight 按列复制1024份] * [encoder_outputs] 16*1024\n",
        "    # attention_weight.shape() 64 16 1; encoder_outputs.shape() 64 16 1024\n",
        "    # attention_weights 实际是length的权重\n",
        "    context_vector = attention_weights * encoder_outputs\n",
        "    \n",
        "    # context_vector.shape: (batch_size, units) 64 1024\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_model = BahdanauAttention(units = 10)\n",
        "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
        "\n",
        "print(\"attention_results.shape:\", attention_results.shape)\n",
        "print(\"attention_weights.shape:\", attention_weights.shape)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}