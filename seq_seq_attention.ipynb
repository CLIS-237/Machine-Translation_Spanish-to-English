{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "seq_seq_attention.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1bJC27ibhXVGWPSvgXOfdrdIvfyjx6W_8",
      "authorship_tag": "ABX9TyOH5G44zKnl/YjK6lSKzmF9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SunshyPiKaChew/seq2seq_attention/blob/master/seq_seq_attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnWPdM__tQJc"
      },
      "source": [
        "#!pip install tensorflow==2.0.0-beta1\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from tensorflow import keras"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMzpjEWgtQXz",
        "outputId": "6441c40d-f6f5-40b6-9183-ebcd12ff8fdc"
      },
      "source": [
        "! /opt/bin/nvidia-smi"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: /opt/bin/nvidia-smi: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLixUxKutcl2",
        "outputId": "5b296e9c-6c53-414b-96e8-e5f496232ba6"
      },
      "source": [
        "print(tf.__version__)\n",
        "print(sys.version_info)\n",
        "for module in mpl, np, pd, sklearn, tf, keras:\n",
        "  print(module.__name__, module.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n",
            "sys.version_info(major=3, minor=7, micro=10, releaselevel='final', serial=0)\n",
            "matplotlib 3.2.2\n",
            "numpy 1.19.5\n",
            "pandas 1.1.5\n",
            "sklearn 0.22.2.post1\n",
            "tensorflow 2.4.1\n",
            "tensorflow.keras 2.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVyjESk2tzDv"
      },
      "source": [
        "# 1. preprocessing data\n",
        "# 2. build model\n",
        "# 2.1 encoder\n",
        "# 2.2 attetion\n",
        "# 2.3 decoder\n",
        "# 3. evaluation\n",
        "# 3.1 given sentence. return translate results\n",
        "# 3.2 visualize results (attention)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6hEAeCUvzlo",
        "outputId": "614ace63-826d-418d-de8e-94dd7f589ce9"
      },
      "source": [
        "# unicode2ascii 去掉西班牙语的重音\n",
        "import unicodedata\n",
        "def unicode_to_ascii(s):\n",
        "  # normalize 的 NFD 方法，如果一个unicode值包含多个字符，那么把他拆开，例如e和重音分开\n",
        "  # 'Mn' 重音的分类标志\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n",
        "en_sentence = 'Then what?'\n",
        "sp_sentence = '¿Entonces qué?'\n",
        "\n",
        "print(unicode_to_ascii(en_sentence))\n",
        "print(unicode_to_ascii(sp_sentence))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Then what?\n",
            "¿Entonces que?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSq4oDIhypbS",
        "outputId": "71a93cde-d33b-4dcc-e014-a300989333f6"
      },
      "source": [
        "# 字符串预处理\n",
        "import re\n",
        "def preprocess_sentence(s):\n",
        "  s = unicode_to_ascii(s.lower().strip())\n",
        "  # [] 匹配操作 () 替换操作 前后加空格\n",
        "  s = re.sub(r\"([?.!,¿])\", r\" \\1 \", s)\n",
        "  # 将一个或多个空格替换为一个空格\n",
        "  s = re.sub(r'[\" \"]+', \" \", s)\n",
        "  # 除了字母和标点符号都替换为空格\n",
        "  s = re.sub(r'[^a-zA-Z?.!,¿]', \" \", s)\n",
        "  # 去掉前后的空格\n",
        "  s = s.rstrip().strip()\n",
        "  # 添加前后特殊字符\n",
        "  s = '<start> ' + s + ' <end>'\n",
        "  return s\n",
        "\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence))\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> then what ? <end>\n",
            "<start> ¿ entonces que ? <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1orHnX7vNGD",
        "outputId": "574753f9-7e22-4a2a-da42-8fb1ebf07549"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')\n",
        "#en_spa_file_path = '/content/drive/MyDrive/Colab/chapter_10/data_spa_en/spa.txt'\n",
        "\n",
        "# 非挂载方式\n",
        "#en_spa_file_path = '/content/train_local/spa.txt'\n",
        "\n",
        "# 尝试2\n",
        "!mkdir ./data\n",
        "!cp ./drive/MyDrive/Colab\\ Notebooks/spa.zip ./data/\n",
        "!cd ./data && unzip spa.zip\n",
        "en_spa_file_path = './data/spa.txt'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  spa.zip\n",
            "  inflating: spa.txt                 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5q1aGPzXjdGQ",
        "outputId": "e87a245a-efb4-41ea-9c7f-7784e03bc82b"
      },
      "source": [
        "def parse_data(filename):\n",
        "  # 根据回车分割数据中的每一行\n",
        "  lines = open(filename, encoding='UTF-8').read().strip().split('\\n')\n",
        "  # 根据制表符分割西班牙文和英文\n",
        "  sentence_pairs = [line.split('\\t') for line in lines]\n",
        "  preprocessed_sentence_pairs = [\n",
        "    (preprocess_sentence(en), preprocess_sentence(sp)) for en,sp in sentence_pairs]\n",
        "  return zip(*preprocessed_sentence_pairs)\n",
        "\n",
        "en_dataset, sp_dataset = parse_data(en_spa_file_path)\n",
        "print(en_dataset[-1])\n",
        "print(sp_dataset[-1])\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sF7iAng9yziM",
        "outputId": "46fe0ee7-d60c-48e2-f7a3-1921b6de668b"
      },
      "source": [
        "# 文本式数据转化为ID式数据\n",
        "def tokenizer(lang):\n",
        "  lang_tokenizer = keras.preprocessing.text.Tokenizer(num_words = None, filters='', split=' ')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  # Padding\n",
        "  tensor = keras.preprocessing.sequence.pad_sequences(tensor, padding = 'post')\n",
        "  return tensor, lang_tokenizer\n",
        "\n",
        "input_tensor, input_tokenizer = tokenizer(sp_dataset[0:30000])\n",
        "output_tensor, output_tokenizer = tokenizer(en_dataset[0:30000])\n",
        "\n",
        "def max_length(tensor):\n",
        "  return max(len(t) for t in tensor)\n",
        "\n",
        "max_length_input = max_length(input_tensor)\n",
        "max_length_output = max_length(output_tensor)\n",
        "print(max_length_input, max_length_output)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3at6ZzSc_qZs",
        "outputId": "295226d4-8470-43ef-e5f8-8e71b37ebbfb"
      },
      "source": [
        "# 调用sklearn函数分割数据集\n",
        "from sklearn.model_selection import train_test_split\n",
        "input_train, input_eval, output_train, output_eval = train_test_split(input_tensor, output_tensor, test_size = 0.2)\n",
        "len(input_train),len(input_eval), len(output_train), len(output_eval)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(24000, 6000, 24000, 6000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA5l5F6iApKQ",
        "outputId": "ede68d38-e241-43ed-ca5a-46b46f07d831"
      },
      "source": [
        "def convert(example, tokenizer):\n",
        "  for t in example:\n",
        "    if t != 0:\n",
        "      print('%d --> %s' % (t,tokenizer.index_word[t]))\n",
        "\n",
        "convert(input_train[0],input_tokenizer)\n",
        "print()\n",
        "convert(output_train[0],output_tokenizer)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 --> <start>\n",
            "8 --> no\n",
            "167 --> os\n",
            "5156 --> peleeis\n",
            "3 --> .\n",
            "2 --> <end>\n",
            "\n",
            "1 --> <start>\n",
            "30 --> don\n",
            "12 --> t\n",
            "539 --> fight\n",
            "3 --> .\n",
            "2 --> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J32VumKI-L7N",
        "outputId": "42853fa5-4d71-4982-899c-bb4d3c87baf5"
      },
      "source": [
        "# 比如训练集有50000个样本，而我设定的batch_size是50，也就是说每50个样本才更新一次参数，那么也就意味着一个epoch里会提取1000次bach，\n",
        "# 这样才会把每个样本都提取了一遍，更新了1000次参数。\n",
        "\n",
        "# 这是一个epoch里做的，依次类推，我要设定2000个epoch意味着把这个过程重复2000次。也就是训练集里的每个样本都被提取了2000次。\n",
        "\n",
        "# 生成DataSet\n",
        "def make_dataset(input_tensor, output_tensor, batch_size, epochs, shuffle):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((input_tensor, output_tensor))\n",
        "  if shuffle:\n",
        "    dataset = dataset.shuffle(30000)\n",
        "  dataset = dataset.repeat(epochs).batch(batch_size, drop_remainder = True)\n",
        "  return dataset\n",
        "\n",
        "batch_size = 64;\n",
        "epochs = 20\n",
        "\n",
        "train_dataset = make_dataset(input_train, output_train, batch_size, epochs, True)\n",
        "eval_dataset = make_dataset(input_eval, output_eval, batch_size, epochs, False)\n",
        "\n",
        "# 64 是一个bacth的大小 16 11 分别为输入输出padding之后的大小\n",
        "for x,y in train_dataset.take(1):\n",
        "  print(x.shape)\n",
        "  print(y.shape)\n",
        "  print(x)\n",
        "  print(y)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 16)\n",
            "(64, 11)\n",
            "tf.Tensor(\n",
            "[[   1   28    8 ...    0    0    0]\n",
            " [   1 2660    3 ...    0    0    0]\n",
            " [   1   69 8148 ...    0    0    0]\n",
            " ...\n",
            " [   1   65 5991 ...    0    0    0]\n",
            " [   1 1891   13 ...    0    0    0]\n",
            " [   1  344    8 ...    0    0    0]], shape=(64, 16), dtype=int32)\n",
            "tf.Tensor(\n",
            "[[   1   20   11   34  490    3    2    0    0    0    0]\n",
            " [   1  227   17    3    2    0    0    0    0    0    0]\n",
            " [   1  188   26  857    3    2    0    0    0    0    0]\n",
            " [   1  266   59  126  684    3    2    0    0    0    0]\n",
            " [   1    5    8  107    3    2    0    0    0    0    0]\n",
            " [   1   46   11   36  112    3    2    0    0    0    0]\n",
            " [   1    4   25  268    9   83    3    2    0    0    0]\n",
            " [   1   36  132   33   13   83    3    2    0    0    0]\n",
            " [   1    6   30   12 1525   17    3    2    0    0    0]\n",
            " [   1    4   18   79 1026    3    2    0    0    0    0]\n",
            " [   1  425   49   59 1307    3    2    0    0    0    0]\n",
            " [   1   46   11  125  187    3    2    0    0    0    0]\n",
            " [   1   56  920   19    3    2    0    0    0    0    0]\n",
            " [   1   20  271   40 1403    3    2    0    0    0    0]\n",
            " [   1   28   23 3327    3    2    0    0    0    0    0]\n",
            " [   1   24    6    9 2837    7    2    0    0    0    0]\n",
            " [   1   45    8  130  223    3    2    0    0    0    0]\n",
            " [   1   71   29    6  160    7    2    0    0    0    0]\n",
            " [   1   20  113    8  979    3    2    0    0    0    0]\n",
            " [   1   27   11  273    3    2    0    0    0    0    0]\n",
            " [   1    5  163    4   76  253    3    2    0    0    0]\n",
            " [   1   72    6   55  335    3    2    0    0    0    0]\n",
            " [   1    4   18   13  146  364    3    2    0    0    0]\n",
            " [   1   45 1246   13  839    3    2    0    0    0    0]\n",
            " [   1    4  151  235  287    3    2    0    0    0    0]\n",
            " [   1    4   35  575    3    2    0    0    0    0    0]\n",
            " [   1    6   38  202    9  196    3    2    0    0    0]\n",
            " [   1   16   29  146 1636    3    2    0    0    0    0]\n",
            " [   1   29    6  694  358    7    2    0    0    0    0]\n",
            " [   1    5 1247   45    3    2    0    0    0    0    0]\n",
            " [   1   14    8  103  193    3    2    0    0    0    0]\n",
            " [   1   56   46   17   73    3    2    0    0    0    0]\n",
            " [   1 1096   41    3    2    0    0    0    0    0    0]\n",
            " [   1   42    5   43   45    7    2    0    0    0    0]\n",
            " [   1  755 1550 1111    3    2    0    0    0    0    0]\n",
            " [   1    4  173   50    3    2    0    0    0    0    0]\n",
            " [   1   82   87   12    6  122    7    2    0    0    0]\n",
            " [   1   20   11  130  273    3    2    0    0    0    0]\n",
            " [   1    4   18 1104  410   41    3    2    0    0    0]\n",
            " [   1    5  200  140  139    3    2    0    0    0    0]\n",
            " [   1   13 2648  233    3    2    0    0    0    0    0]\n",
            " [   1  189   11  429    3    2    0    0    0    0    0]\n",
            " [   1   29   28  306    7    2    0    0    0    0    0]\n",
            " [   1   98   17   13   83  530    3    2    0    0    0]\n",
            " [   1    5    8   33   13 2352    3    2    0    0    0]\n",
            " [   1  388 1081    8   19    7    2    0    0    0    0]\n",
            " [   1   14   76   17    9  177    3    2    0    0    0]\n",
            " [   1    4   18  715    3    2    0    0    0    0    0]\n",
            " [   1   30   12  574   17    3    2    0    0    0    0]\n",
            " [   1   10   26 2452  315    3    2    0    0    0    0]\n",
            " [   1    4   18    9  303  275    3    2    0    0    0]\n",
            " [   1   13  460   26   78 1144    3    2    0    0    0]\n",
            " [   1   13 2398    8 4832    3    2    0    0    0    0]\n",
            " [   1    4   29    9  141 1080    3    2    0    0    0]\n",
            " [   1   10   26  223 2280    3    2    0    0    0    0]\n",
            " [   1   28   92  148    3    2    0    0    0    0    0]\n",
            " [   1   53  448    3    2    0    0    0    0    0    0]\n",
            " [   1    4   35   15  112 2002    3    2    0    0    0]\n",
            " [   1    4  221  126    3    2    0    0    0    0    0]\n",
            " [   1    4  135  141   54  126    3    2    0    0    0]\n",
            " [   1   16   29  208  909    3    2    0    0    0    0]\n",
            " [   1    4   18   70  572    3    2    0    0    0    0]\n",
            " [   1    4 1602   13   73    3    2    0    0    0    0]\n",
            " [   1    6   23   34  138    3    2    0    0    0    0]], shape=(64, 11), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWAZqZy289ly"
      },
      "source": [
        "# 超参数定义\n",
        "\n",
        "# 将单词进行编码，编码长度为256\n",
        "embedding_units = 256   \n",
        "# 中间循环神经网络 encoder decoder  \n",
        "units = 1024          \n",
        "# 输入词表长度\n",
        "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
        "# 输出词表长度\n",
        "output_vocab_size = len(output_tokenizer.word_index) + 1"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_BB8LDp50XO",
        "outputId": "2e340bc6-440b-4eaf-b7e6-546637621e46"
      },
      "source": [
        "# 调用子类API\n",
        "class Encoder(tf.keras.Model):\n",
        "  # 初始化函数\n",
        "  def __init__(self, vocab_size, embedding_units, encoding_units, batch_size):\n",
        "    # 调用父类初始化函数\n",
        "    super(Encoder, self).__init__()\n",
        "    # 赋初值\n",
        "    self.batch_size = batch_size;\n",
        "    self.encoding_units = encoding_units;\n",
        "    # 一个规定输入词表大小和输出编码大小的编码器\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
        "    # 每一步的隐层状态： 矩阵 hide_state 最后一步的输出：cell_state\n",
        "    self.gru = keras.layers.GRU(self.encoding_units, return_sequences = True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
        "  def call(self, x, hidden):\n",
        "    # 输入编码\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden)\n",
        "    # 每一步输出和最后一次输出的隐含状态\n",
        "    return output, state\n",
        "\n",
        "  def initialize_hidden_state(self):    \n",
        "    return tf.zeros((self.batch_size, self.encoding_units))\n",
        "\n",
        "encoder = Encoder(input_vocab_size, embedding_units, units, batch_size)\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder.call(x, sample_hidden)\n",
        "print('output_shape')\n",
        "print(sample_output.shape)\n",
        "print('decoder_hidden_shape')\n",
        "print(sample_hidden.shape)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "output_shape\n",
            "(64, 16, 1024)\n",
            "decoder_hidden_shape\n",
            "(64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InQe9MmLXvGB",
        "outputId": "55f868d6-6cb0-4bac-c5e1-688c84cf58f5"
      },
      "source": [
        "class BahdanauAttention(keras.Model):\n",
        "  def __init__(self, units):\n",
        "    # units 全连接层的维度 y = A * x\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = keras.layers.Dense(units)\n",
        "    self.W2 = keras.layers.Dense(units)\n",
        "    self.V = keras.layers.Dense(1)\n",
        "  def call(self, decoder_hidden, encoder_outputs):\n",
        "    # decoder_hidden.shape: (batch_size,units) (64,1024)\n",
        "    # encoder_outputs.shape: (batch_size, length, units) (64,16,1024)\n",
        "    # decoder_hidden_with_time_axis: (batch_size,1,units) (64,1,1024)\n",
        "    # 保证大的维度一致，就可以相加\n",
        "\n",
        "    decoder_hidden_with_time_axis = tf.expand_dims(decoder_hidden, 1)\n",
        "\n",
        "    # before V: (batch_size, length, units) 64 16 10\n",
        "    # after V socre: (batch_size, length, 1)  64 16 1\n",
        "    # self.W1(encoder_outputs) 64 16 10\n",
        "    # self.W2(decoder_hidden_with_time_axis) 64 1 10\n",
        "    # tanh不改变维度\n",
        "    score = self.V(\n",
        "        tf.nn.tanh(\n",
        "            self.W1(encoder_outputs) + self.W2(decoder_hidden_with_time_axis)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # attention_weights.shape: (batch_size, length, 1)\n",
        "    # attention 只和 单词（length） 有关系, 所以只能在length上做softmax\n",
        "    attention_weights = tf.nn.softmax(score, axis = 1)\n",
        "    \n",
        "    # context_vector.shape: (batch_size, length, units) 64 16 1024\n",
        "    # 维度不匹配的相乘(忽略batch_size 维度)\n",
        "    # [attention_weight 按列复制1024份] * [encoder_outputs] 16*1024\n",
        "    # attention_weight.shape() 64 16 1; encoder_outputs.shape() 64 16 1024\n",
        "    # attention_weights 实际是length的权重\n",
        "    context_vector = attention_weights * encoder_outputs\n",
        "    \n",
        "    # context_vector.shape: (batch_size, units) 64 1024\n",
        "    context_vector = tf.reduce_sum(context_vector, axis = 1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "attention_model = BahdanauAttention(units = 10)\n",
        "attention_results, attention_weights = attention_model(sample_hidden, sample_output)\n",
        "\n",
        "print(\"attention_results.shape:\", attention_results.shape)\n",
        "print(\"attention_weights.shape:\", attention_weights.shape)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention_results.shape: (64, 1024)\n",
            "attention_weights.shape: (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWZJpQQ16SRs",
        "outputId": "0a06d2ea-5356-4122-b447-13021cb97c74"
      },
      "source": [
        "class Decoder(keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_units, decoding_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.decoding_units = decoding_units\n",
        "    self.embedding = keras.layers.Embedding(vocab_size, embedding_units)\n",
        "    self.gru = keras.layers.GRU(self.decoding_units, return_sequences= True, return_state = True, recurrent_initializer = 'glorot_uniform')\n",
        "    self.fc = keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(decoding_units)\n",
        "  \n",
        "  def call(self, x, hidden, encoding_outputs):\n",
        "    # context_vector.shape: (batch_size, units) 64 1024\n",
        "    context_vector, attention_weights = self.attention(hidden, encoding_outputs)\n",
        "    # before embedding: x.shape: (batch_size,1) 64 1 一步编码，x长度是1不是16\n",
        "    x = self.embedding(x)\n",
        "    # after embedding: x.shape: (batch_size, 1, embedding_units) 64, 1, 256\n",
        "    # 按照最后一个维度拼接\n",
        "    combined_x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n",
        "    # output.shape: (batch_size, 1, decoding_units) 64 1 1024\n",
        "    # state.shape: (batch_size, decoding_units) 64 1024\n",
        "    output, state = self.gru(combined_x)\n",
        "\n",
        "    # output.shape: (batch_size, decoding_units)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    # output.shape： (batch_size, output_vocab_size)\n",
        "    output = self.fc(output)\n",
        "\n",
        "    return output, state, attention_weights\n",
        "\n",
        "decoder = Decoder(output_vocab_size, embedding_units, units, batch_size)\n",
        "outputs = decoder(tf.random.uniform((batch_size, 1)), sample_hidden, sample_output)\n",
        "decoder_output, decoder_hidden, decoder_aw = outputs\n",
        "print(\"decoder_output.shape: \", decoder_output.shape)\n",
        "print(\"decoder_hidden.shape: \", decoder_hidden.shape)\n",
        "# attention 用的是sample数据，所以和16有关系\n",
        "print(\"decoder_attention_weights.shape: \", decoder_aw.shape)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "decoder_output.shape:  (64, 4935)\n",
            "decoder_hidden.shape:  (64, 1024)\n",
            "decoder_attention_weights.shape:  (64, 16, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jx1H_qxqQA1l"
      },
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "# from_logits: True->没经过softmax False->经过softmax\n",
        "# reduction : mask 之后聚合\n",
        "loss_object = keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = 'none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # padding部分为0，非padding部分为1\n",
        "  mask = tf.math.logical_not(tf.math.equal(real,0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  # 精度转化\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crCz78WPTSny"
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, encoding_hidden):\n",
        "  loss = 0\n",
        "  with tf.GradientTape() as tape:\n",
        "    encoding_output, encoding_hidden = encoder(inp, encoding_hidden)\n",
        "    decoding_hidden = encoding_hidden\n",
        "    # eg:<start> I am here <end>\n",
        "    # 1. <start> -> I\n",
        "    # 2. I -> am\n",
        "    # 3. am -> here\n",
        "    # 4. here -> <end>\n",
        "    for t in range(0, targ.shape[1] - 1):\n",
        "      # batch_size 1\n",
        "      decoding_input = tf.expand_dims(targ[:,t], 1)\n",
        "      prediction, decoding_hidden, _ = decoder(decoding_input, decoding_hidden, encoding_output)\n",
        "      loss += loss_function(targ[:,t+1], prediction)\n",
        "    \n",
        "  batch_loss = loss/int(targ.shape[0])\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "  return batch_loss"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pSyjvG7RQnd",
        "outputId": "987d06f9-432b-40b3-f3db-0588eb99bde6"
      },
      "source": [
        "epochs = 1\n",
        "step_per_epoch = len(input_tensor) // batch_size\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  start = time.time()\n",
        "  encoding_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # enumerate的返回形式 (batch, (inp, targ))\n",
        "  for(batch, (inp, targ)) in enumerate(train_dataset.take(step_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, encoding_hidden)\n",
        "    total_loss += batch_loss\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1, batch, batch_loss.numpy()))\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1, total_loss / step_per_epoch))\n",
        "  print('Time take for 1 epoch {} sec\\n'.format(time.time()-start))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.7973\n",
            "Epoch 1 Batch 100 Loss 0.3625\n",
            "Epoch 1 Batch 200 Loss 0.3421\n",
            "Epoch 1 Batch 300 Loss 0.2736\n",
            "Epoch 1 Batch 400 Loss 0.2584\n",
            "Epoch 1 Loss 0.3310\n",
            "Time take for 1 epoch 1738.6326332092285 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lMwkmq5BYys"
      },
      "source": [
        "def evalute(input_sentence):\n",
        "  attention_matrix = np.zeros((max_length_output, max_length_input))\n",
        "  input_sentence = preprocess_sentence(input_sentence)\n",
        "\n",
        "  inputs = [input_tokenizer.word_index[token] for token in input_sentence.split(' ')]\n",
        "  inputs = keras.preprocessing.sequence.pad_sequences([inputs], maxlen = max_length_input, padding = 'post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  results = ''\n",
        "  encoding_hidden = tf.zeros((1,units))\n",
        "\n",
        "  encoding_outputs, encoding_hidden = encoder(inputs, encoding_hidden)\n",
        "  decoding_hidden = encoding_hidden\n",
        "\n",
        "  # eg: <start> -> A -> B -> ...\n",
        "  # decoding_input.shape (1,1) 为batch扩展维度\n",
        "  decoding_input = tf.expand_dims([output_tokenizer.word_index['<start>']],0)\n",
        "  \n",
        "  for t in range(max_length_output):\n",
        "    predictions, decoding_hidden, attention_weights = decoder(\n",
        "        decoding_input, decoding_hidden, encoding_outputs)\n",
        "    # attention_weights.shape: (batch_size, input_length, 1) (1,16,1)\n",
        "    attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "    # attention_weights.shape: 1 16\n",
        "    attention_matrix[t] = attention_weights.numpy()\n",
        "\n",
        "    # predictions.shape: [batch_size, vocab_size] (1,4935)\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    results += output_tokenizer.index_word[predicted_id] + ' '\n",
        "\n",
        "    if output_tokenizer.index_word[predicted_id] == '<end>':\n",
        "      return results, input_sentence, attention_matrix\n",
        "    # decoding_input.shape (1,1) 为batch扩展维度\n",
        "    decoding_input = tf.expand_dims([predicted_id],0)\n",
        "  return results, input_sentence, attention_matrix\n",
        "\n",
        "# 绘制attention_matrix\n",
        "def plot_attention(attention_matrix, input_sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1,1,1)\n",
        "  ax.matshow(attention_matrix, cmap = 'viridis')\n",
        "  font_dict = {'fontsize': 14}\n",
        "  ax.set_xticklabels([''] + input_sentence,\n",
        "              fontdict = font_dict, rotation = 90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence,\n",
        "              fontdict = font_dict,)\n",
        "  plt.show()\n",
        "  \n",
        "def translate(input_sentence):\n",
        "  results, input_sentence, attention_matrix = evalute(input_sentence)\n",
        "  print(\"Input: %s\" % (input_sentence))\n",
        "  print(\"Predicted translation: %s\" % (results))\n",
        "  # 去除padding部分\n",
        "  attention_matrix = attention_matrix[:len(results.split(' ')),\n",
        "                      :len(input_sentence.split(' '))]\n",
        "  plot_attention(attention_matrix, input_sentence.split(' '), results.split(' '))"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "id": "aY9VjIyKLcRP",
        "outputId": "e6f42727-ba73-4a8b-d94c-7affe7521bc8"
      },
      "source": [
        "translate(u'hace mucho frío aquí.')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: this is a good late . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}